{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from tqdm import tqdm \n",
    "from octis.preprocessing.preprocessing import Preprocessing\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from Code.TNTM.TNTM_SentenceTransformer import TNTM_SentenceTransformer\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import functools\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code allows to skip the SSLerror when useing sentenceTransformer\n",
    "import requests\n",
    "from huggingface_hub import configure_http_backend\n",
    "\n",
    "def backend_factory() -> requests.Session:\n",
    "    session = requests.Session()\n",
    "    session.verify = False\n",
    "    return session\n",
    "\n",
    "configure_http_backend(backend_factory=backend_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(41)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'current device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_path = \"Data/Preprocessed_Data\"\n",
    "file_path = os.path.join(cleaned_data_path, \"email2words.pickle\")\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of unique words\n",
    "vocab = []\n",
    "for each_lst in corpus:\n",
    "    vocab.extend(each_lst)\n",
    "\n",
    "vocab = list(set(vocab))\n",
    "print(f\"Total number of unique words: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained sentenceTransformer for mulit language\n",
    "modelPath = \"Data/Bert_Embedder\"\n",
    "embeddings_model = SentenceTransformer(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # embeddings of unique words\n",
    "# unique_words_embeddings = [embeddings_model.encode(word) for word in tqdm(vocab)]\n",
    "# unique_words_embeddings = torch.Tensor(unique_words_embeddings)\n",
    "\n",
    "# cleaned_data_path = \"Data/Preprocessed_Data\"\n",
    "# file_path = os.path.join(cleaned_data_path, \"vocab_embeddings.pickle\")\n",
    "\n",
    "# with open(file_path, \"wb\") as file:\n",
    "#     pickle.dump(unique_words_embeddings, file)\n",
    "# unique_words_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_path = \"Data/Preprocessed_Data\"\n",
    "file_path = os.path.join(cleaned_data_path, \"vocab_embeddings.pickle\")\n",
    "\n",
    "with open(file_path, \"rb\") as file:\n",
    "    vocab_embeddings = pickle.load(file)\n",
    "vocab_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings of unique words\n",
    "cleaned_data_path = \"Data/Preprocessed_Data\"\n",
    "file_path = os.path.join(cleaned_data_path, \"email_embeddings.pickle\")\n",
    "\n",
    "with open(file_path, \"rb\") as file:\n",
    "    sentence_embedding = pickle.load(file)\n",
    "sentence_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tntm = TNTM_SentenceTransformer(\n",
    "    n_topics = 20,\n",
    "    save_path = f\"Data/Auxillary_Data/{20}_topics\",\n",
    "    n_dims = 11,\n",
    "    n_hidden_units = 200,\n",
    "    n_encoder_layers = 2,\n",
    "    enc_lr = 1e-3,\n",
    "    dec_lr = 1e-3,\n",
    "    n_epochs = 1000,\n",
    "    batch_size = 128,\n",
    "    #batch_size = 256,\n",
    "    dropout_rate_encoder = 0.3,\n",
    "    prior_variance =  0.995, \n",
    "    prior_mean = None,\n",
    "    n_topwords = 200,\n",
    "    device = device, \n",
    "    validation_set_size = 0.2, \n",
    "    early_stopping = True,\n",
    "    n_epochs_early_stopping = 15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tntm.fit(\n",
    "              corpus              = corpus,\n",
    "              vocab               = vocab, \n",
    "              word_embeddings     = vocab_embeddings,\n",
    "              document_embeddings = sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = result[1]\n",
    "# normalize weights for each corresponding unique word\n",
    "normalize_weights = weights/weights.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top-k words for each cluster\n",
    "top_k = 5\n",
    "top_words_per_cluster = []\n",
    "for cluster_idx in range(normalize_weights.shape[0]):  # Iterate over clusters\n",
    "    # Get weights for all words in the cluster\n",
    "    word_weights = normalize_weights[cluster_idx]\n",
    "    \n",
    "    # Get indices of the top-k words\n",
    "    top_k_indices = word_weights.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    # Map indices to words using resulttt[0]\n",
    "    top_words = [result[0][cluster_idx][i] for i in top_k_indices]\n",
    "    top_words_per_cluster.append(top_words)\n",
    "\n",
    "# Print the top-k words for each cluster\n",
    "for cluster_idx, words in enumerate(top_words_per_cluster):\n",
    "    print(f\"Cluster {cluster_idx + 1}: {words}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".bertopicenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
